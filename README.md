Лабораторная работа #3.
====
Изучение влияния параметра “темп обучения” на процесс обучения нейронной сети на примере решения задачи классификации Oregon Wildlife с использованием техники обучения Transfer Learning
---
1)С использованием техники обучения Transfer Learning обучить нейронную сеть EfficientNet-B0 (предварительно обученную на базе изображений imagenet) для решения задачи классификации изображений Oregon WildLife с использованием фиксированных темпов обучения 0.1, 0.01, 0.001, 0.0001
---
Для этого задания был изменен темп обучения следующим образом:
```
optimizer=tf.optimizers.Adam(lr=0.1)
optimizer=tf.optimizers.Adam(lr=0.01)
optimizer=tf.optimizers.Adam(lr=0.001)
optimizer=tf.optimizers.Adam(lr=0.0001)
```
Графики обучения для нейронной сети EfficientNetB0(предварительно обученной на базе изображений imagenet) с использованием фиксированных темпов обучения 0.1, 0.01, 0.001, 0.0001:
---

***Линейная диаграмма точности:***
![XwXR0L51aqE](https://user-images.githubusercontent.com/58634989/111905068-ecd4f080-8a5a-11eb-8c3b-e1c5b9b5807f.jpg)

<img src="./epoch_categorical_accuracy_1_part.svg">

***Линейная диаграмма потерь:*** 
![zwugauPgwgM](https://user-images.githubusercontent.com/58634989/111905080-f9f1df80-8a5a-11eb-8efa-138109dafff9.jpg)

<img src="./epoch_loss_1_part.svg">  

***Анализ результатов:***
Из графиков видно, что наилучшее значение на фиксированном темпе обучения - **0.001**, на валидационном наборе данных метрика точности = **89.13%**, ошибки при этом = **0.2657**.

2)Реализовать и применить в обучении следующие политики изменения темпа обучения, а также определить оптимальные параметры для каждой политики:
---
**a. Пошаговое затухание (Step Decay)**

**b. Экспоненциальное затухание (Exponential Decay)**

Для пошагового затухания использовалась функция:
```
def step_decay(epoch,lr):
  initial_lrate = 0.001
  drop = 0.5
  epochs_drop = 5.0
  lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))
  return lrate
  ```
где:
* `initial_lrate = 0.001` - означает начальный темп обучения 
* `drop = 0.5` - снижение скорости обучение в 2 раза 
* `epochs_drop = 5.0` - каждые 5 эпох происходит снижение скорости обучения 

Для экспоненциального затухания использовалась функция:
```
def exp_decay(epoch,lr):
  initial_lrate = 0.001
  k = 0.1
  lrate = initial_lrate * math.exp(-k*epoch)
  return lrate
```

Также необходимо передать ***LearningRateScheduler(Планировщик скорости обучения)*** в ***callbacks(обратный вызов)*** - объект, который может выполнять действия на различных этапах обучения (например, в начале или в конце эпохи, до или после одной партии и т. д.).
```
callbacks=[
      tf.keras.callbacks.TensorBoard(log_dir),
      LearningRateScheduler(step_decay)
    ]
```
 Была импортирована библиотека math
 ```
 import math
 ```
Графики обучения для нейронной сети EfficientNetB0(предварительно обученной на базе изображений imagenet) с использованием следующих политик изменения темпа обучения(Пошаговое затухание (Step Decay),Экспоненциальное затухание (Exponential Decay)):
---
-Step Decay-
---
***Линейная диаграмма точности для политики изменения темпа обучения StepDecay:***
![K0QcTAZH64s](https://user-images.githubusercontent.com/58634989/111903899-5225e300-8a55-11eb-8a1a-6adf427d4a4f.jpg)

<img src="./epoch_categorical_accuracy_step.svg">

***Линейная диаграмма потерь для политики изменения темпа обучения StepDecay:*** 
![ghkIBgZn-z8](https://user-images.githubusercontent.com/58634989/111903972-944f2480-8a55-11eb-8308-072dd2da3335.jpg)

<img src="./epoch_loss_step.svg">  

***Анализ результатов:***
Из графиков видно, что на валидационном наборе данных лучшее значение метрики точности для `initial_lrate=0.01_drop=0.4_epochs_drop=5` параметров и = **89.51%**, и уже на 42 эпохе график выходит на плато, ошибки при этом составляют **0.2895**, однако ошибок меньше при `initial_lrate=0.001_drop=0.4_epochs_drop=5` параметрах и = **0.2692**, точность же при этом отличается на **2.18%**, определим `initial_lrate=0.01_drop=0.4_epochs_drop=5` параметры как оптимальные.

-Exponential Decay-
---
***Линейная диаграмма точности для политики изменения темпа обучения ExpDecay:***
![0MBxjDSbaP0](https://user-images.githubusercontent.com/58634989/111904113-37a03980-8a56-11eb-9cf3-5262fd72c1b1.jpg)


<img src="./epoch_categorical_accuracy_exp.svg">

***Линейная диаграмма потерь для политики изменения темпа обучения ExpDecay:*** 
![xeze8_4qjRc](https://user-images.githubusercontent.com/58634989/111904119-3ff87480-8a56-11eb-9b39-aea3e6044208.jpg)

<img src="./epoch_loss_exp.svg">  

***Анализ результатов:***
Из графика метрики точности видно, что лучшее значение достигается при `initial_lrate=0.1_k=0.3` параметрах, но ошибки при этом слишком велики, оптимальным результатом по точности и потерям можно определить такие `initial_lrate=0.01_k=0.5` параметры, при которых точность = **88.92%**, ошибки = **0.2319** 

-Вывод-
---
***Линейная диаграмма точности для наилучших результатов всех экспериментов:***

![D6IvSuET8ZM (1)](https://user-images.githubusercontent.com/58634989/111906062-df6e3500-8a5f-11eb-93b4-29898469a800.jpg)

<img src="./epoch_categorical_accuracy_comp.svg">

***Линейная диаграмма потерь для наилучших результатов всех экспериментов:*** 

![4Twt9itmBSU](https://user-images.githubusercontent.com/58634989/111906071-e72dd980-8a5f-11eb-884f-9f33e2b3fc62.jpg)

<img src="./epoch_loss_comp.svg">  

***Вывод:***
Из графиков можно отметить, что лучшим по точности получился эксперимент с политикой пошагового затухания(Step Decay) при `initial_lrate=0.01_drop=0.4_epochs_drop=5` параметрах, что больше, чем у фиксированного темпа обучения **0.001** на **0.38%** и чем у экспоненциального заухания на **0.59%**, меньше всего ошибок у эксперимента с политикой экспоненциального затухания(Exp Decay) при `initial_lrate=0.01_k=0.5` = **0.2319**, что меньше, чем у фиксированного темпа обучения **0.001** на **0.0338** и чем у пошагового заухания на **0.0576**, можно сделать вывод, что лучший(оптимальный) результат показал фиксированный темп обучения, поскольку на обоих графиках его значение что-то среднее от обоих политик изменения темпа обучения 





